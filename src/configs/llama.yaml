defaults:
    - model: llama
    - writer: wandb
    - metrics: no_metrics
    - datasets: open_web_text
    - dataloader: main
    - transforms: main
    - _self_
optimizer:
    _target_: torch.optim.AdamW
    lr: 3e-4
    betas: [0.9, 0.95]
lr_scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    gamma: 0.9
    step_size: ${trainer.epoch_len}
loss_function:
    _target_: src.loss.CrossEntropyLoss
accelerator:
    _target_: accelerate.Accelerator
    mixed_precision: fp16
    gradient_accumulation_steps: 16
trainer:
    log_step: 20
    n_epochs: 50
    epoch_len: 100
    device_tensors: ["src", "tgt", "attn_mask", "pad_mask"] # which tensors should be on device (ex. GPU)
    resume_from: null # null or path to the checkpoint dir with *.pth and config.yaml
    device: auto # device name or "auto"
    override: True # if True, will override the previous run with the same name
    monitor: "max test_Accuracy" # "off" or "max/min metric_name", i.e. our goal is to maximize/minimize metric
    save_period: 5 # checkpoint each save_period epochs in addition to the best epoch
    early_stop: ${trainer.n_epochs} # epochs for early stopping
    save_dir: "saved"
    seed: 1
